<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.12 (457935)"/><meta name="author" content="rociocprimo@gmail.com"/><meta name="created" content="2018-07-16 12:39:18 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-08-21 10:58:49 +0000"/><title>Archive SOCAT in Pangaea</title></head><body><div><span style="font-style: italic; text-decoration: underline;">Note</span><span style="font-style: italic;">: when working with Mac in general, Excel, TextEdit… careful with the encoding, the double quotes (must be straight), end-of-line (^M vs \r), number/text formats for expocodes, special characters… May need to tweak Pangaea Tools encoding from Mac to UTF-8. Make a habit of standardizing, specially the end-of-line: terminal commands do not recognize ^M and scripts can crash.</span></div><div><br/></div><div>Most of the preparation documents and necessary files will be in the GDrive folder <a href="https://drive.google.com/open?id=1IjwUghN2ZD0lIaZnnydOwOt1Vj5vX181" rev="en_rl_minimal">https://drive.google.com/open?id=1IjwUghN2ZD0lIaZnnydOwOt1Vj5vX181</a></div><div>Structure of the main datasets: the parent includes the New and Updated events only (no excluded or suspended). The metadata dataset contains information about the whole collection.</div><div><br/></div><div><span style="font-weight: bold;">Prepare summary tables</span></div><div>Summary of New and Updated: <a href="https://drive.google.com/open?id=1ncM8UP9WBUfaJEUw3Gsl58io7mhK_NlOk_t4ipHctN4" rev="en_rl_minimal">https://drive.google.com/open?id=1ncM8UP9WBUfaJEUw3Gsl58io7mhK_NlOk_t4ipHctN4</a></div><div><br/></div><div>The working sheet is called with_calc. Once all the info is filled, copy the content to NU_SOCATv- as copy values only. BasisPangaea and StaffPangaea may have info about the names and ID’s (to be implemented).</div><div>The starting point is the output from the QC Editor of the SOCATv6 data, and some fields are automatically filled using that information. The script <span style="color: rgb(0, 144, 81);">extract_first_last_median_position.sh</span> provides the first, last and average (called median in Pangaea) position of cruise tracks. We need to find the:</div><ul><li><div>URI for the source datasets (e.g. the ones we use from OCADS), the</div></li><li><div>IDs of the Basis, the Staff and their Institution, the device abbreviation and ID</div></li><li><div>Campaign ID if it exists already. Some campaigns may have already been created, check that first. It's usually the case with German research vessels (Polarstern, Meteor, Maria S. Merian…). In that case, we use the campaign label given.</div></li><li><div>For updated datasets, the Campaign, Event, ID of the last version of the Dataset and its corresponding SOCAT version. If we gather the datasets IDs (the number after the dot in the DOI), we can use the script <span style="color: rgb(0, 144, 81);">extract_Pangaea_updateddatasetsinfo.sh</span> to extract some of the information.</div></li></ul><div>It is not a static table; it’s being filled with info about Campaign, Events, References, etc… as we create them in Pangaea. We “freeze” the spreadsheet (NU_SOCATv-) and export to a tab-separated .txt right before creating the Datasets import files, as the scripts make use of that information.</div><div><br/></div><div><span style="font-weight: bold;">Open issue</span></div><div>Open ticket in the Pangaea issue tracker: <a href="https://issues.pangaea.de/">https://issues.pangaea.de/</a>, with label “Bjerknes".</div><div>In the description we state the total number of datasets, and specify how many are new/updated/unchanged, in order to keep track. Once they are created, add links to the parent and the metadata datasets. All communication with the Pangaea staff (mostly Stefi) will be done through the ticket. It’s good practice also to add comments about progress milestones.</div><div><br/></div><div><span style="font-weight: bold;">Create IDs for Staff and Basis</span></div><div>There may be Staff (PIs) and Basis not present in the Pangaea database, that need to be created first, as the Campaign import form needs the information. They have to be imported manually one by one. Important to include ORCID (Staff) and IMO number (Basis) if available.</div><div><br/></div><div><span style="font-weight: bold;">Import Campaigns and Events</span></div><div>For <span style="text-decoration: underline;">new</span> datasets: we create all the events, with the structure <span style="font-style: italic;">expocode-track/moor</span> depending on whether the data come from a ship or a buoy. New campaigns have the <span style="font-style: italic;">expocode</span> as label.</div><div>For <span style="text-decoration: underline;">updated</span> datasets, both Campaign and Event have been created already in previous versions. We find their IDs.</div><div><br/></div><div>Use the import forms, to batch import all of them. Import first the Campaigns, then the Events. Notes:</div><ul><li><div>It is possible to add the <span style="font-style: italic;">OptionalLabel</span> column to the Campaign form. Not used for ships, but in moorings it contains the name of the mooring. E.g.: “316420140906" would be the Campaign<span style="font-style: italic;"> Label</span>, while "TAO125W_0N_Sep2014_Mar2015" would be the <span style="font-style: italic;">OptionalLabel</span></div></li><li><div>Moorings have Basis ID 4638, except the PIRATA buoys, which have their own: 4959.</div></li><li><div>Event <span style="font-style: italic;">Device</span> is the abbreviation: MOORY or CT (underwater cruise track measurements). <span style="font-family: 'Times New Roman';">It exists a device called Underway pCO2 (UWPCO2, ID 38791), it has ~25 events associated, all Polarstern cruises, but no datasets, it seems.</span></div></li><li><div>Moorings don’t need <span style="font-style: italic;">LatitudeEvent2 / LongitudeEvent2</span>, they can remain empty.</div></li></ul><div><br/></div><div><span style="font-weight: bold;">Import References</span></div><div>Can be done before, after or simultaneously to the previous imports. Has to be done before generating the Datasets import files. </div><div>The references to import are the Description, Release poster and Fair Data Use for the current version + SOCAT bundles of each dataset. They need to be stored in the Pangaea Store first, so we can provide a handle. Request a subtask (usually Stefi will do it). We need handles for:</div><ul><li><div>The whole SOCATv- collection in original SOCAT format with documentation as a unique .zip file to be downloadable from the parent dataset.</div></li><li><div>The individual bundle folders to download from the individual datasets landing pages. They are referred as “Other version"</div></li></ul><div>Once we have the list of handles (URI), we create the References import file with the title and URI; the New/Updated overview has a couple of columns that will autofill the title. Do not add author, nor year. When leaving the year field empty, 4D will autofill to 0 (???), and the title of the reference will be a clickable link in the landing page. Afterwards, extract the Pangaea IDs of the individual bundle references and add them to the New and Updated overview.</div><div><br/></div><div>References in parent dataset:</div><div><img src="Archive%20SOCAT%20in%20Pangaea.html.resources/1F598786-AD84-4B8B-B7DF-C61BA4C8A669.png" height="490" width="2184"/><br/></div><div>References in the individual datasets:</div><div><img src="Archive%20SOCAT%20in%20Pangaea.html.resources/6EE3A4EE-D878-4F2A-8E47-BA57F8E3B9B5.png" height="290" width="2062"/><br/></div><div><br/></div><div><br/></div><div><span style="font-weight: bold;">“Dummy” dataset and create the parent</span></div><div>It is not possible to create a parent dataset without a child, but having the ID of the SOCAT parent will make much easier the import of datasets. Create a dummy dataset and link it to the parent. Later on, you can delete the dataset, or update it with a proper SOCAT data file.</div><div><br/></div><div><span style="font-weight: bold;">Generation of the datasets import files</span></div><div>We’ll take advantage of the fact that all SOCAT files have the same structure (i.e. number of columns) regardless of content. The MATLAB script <span style="color: rgb(0, 144, 81);">pangaeadatasets.m</span> will create datafiles that group data from the same Basis and New or Updated status, and makes them Pangaea compliant: adds event label column, the datetime in ISO format, longitude as +-180 and empty fields instead of NaN, standard parameter names and units. The script is still under development, and there are exceptions here and there depending on particular cases for particular versions, revise the script before using. It needs that the bundles folder is fully unzipped and two text files with the expocodes of New and Updated datasets, one per line, no header. The script will probably change for future versions. Remember to standardize to UTF (see <span style="font-style: italic;">Note).</span></div><div><br/></div><div>Next, we fire up the Split2Events tool. We add all the datasets generated by the MATLAB script and choose “Create import file…(F5)”. “Create metadata template… (F4)” works, too. We fill the info: make sure that the project (SOCAT, 4143), parent ID and common references (“Related to” paper for v3, “Further details” the Fair Data Use, Description and Release Poster). The rest of the fields can be filled with “dummy” data, as they will be changed later on by a script. In the pictures below, the fields in green need to be correct, in red, we can use “filler”. If the field is not marked, do not fill it.</div><div><br/></div><div><img src="Archive%20SOCAT%20in%20Pangaea.html.resources/C25CF254-3FCD-4DA5-8B12-260F5357CF9B.png" height="1396" width="1854"/><br/></div><div><br/></div><div><img src="Archive%20SOCAT%20in%20Pangaea.html.resources/5559E348-0670-402F-83D3-B90428606681.png" height="1398" width="1856"/><br/></div><div><br/></div><div>And create the metadata template(s). The script <span style="color: rgb(0, 144, 81);">EditSplit2EventMetadata_SOCATv6.sh</span> needs the metadata files generated and a textfile called Basis_PI.txt that contains a tab-separated list of Basis ID’s and their corresponding PI ID, without header. This approach is a bit brute force because not all datasets from the same basis will have necessarily the same PI, but this is nonetheless corrected later in the dataset import files themselves. The script also adds comments, sets methods and different PIs (not_given for geocodes, Are Olsen for fCO2_rec) where necessary. </div><div><br/></div><div>Open Split2Events again, drag the per basis files generated by MATLAB again and select “Create import file…(F5)”. Click OK and we’ll be prompted to chose a metadata file. Pick one, it does not matter much, as we’ll edit the files generated by Split2Events and the crucial info from the metadata file will be about the parameters, which are all the same for all the datasets.</div><div><br/></div><div>Split2Event will generate one datafile per event, with a JSON metadata header. The script <span style="color: rgb(0, 144, 81);">EditSplit2EventData_SOCATv6.sh</span> uses information from the New and Updated summary file to write the correct PI(s), institutions, comments (i.e. flags), other version references (i.e. bundles), dataset title, export filename, source dataset if applicable. Once this script has run, the files are ready to import in 4D. </div><div><br/></div><div><span style="font-weight: bold;">Data archive</span></div><div>For the first, “dummy” dataset, either delete it (do it at the end, so the parent does not end up without children) or update it in the Pangaea sense with a valid dataset.</div><div>Import the datasets in batches of 10-20. It’s a bit arbitrary when 4D will get stuck, so go by feel. Though they should work fine, it is possible that we spot errors while importing; usually it’s because of some error in the JSON header. Going by the experience in v6, the summary txt file may have extra spaces, curve double quotes and other  small details carried over from Excel or other editors. Specially after the first batch, and later randomly, open the imported datasets in 4D and their URL to check that they are importing correctly. </div><div><br/></div><div>The SOCAT updated files require an additional step. In the last version dataset, add a “New version” reference to the current version of the dataset.</div><div><br/></div><div>To the <span style="text-decoration: underline;">parent</span> dataset: </div><ul><li><div>Add the keywords BCCR and SOCATv-. </div></li><li><div>Add, via subtask because the files are quite big, two “Other version” files: </div></li><ul><li><div>a zip file containing ALL SOCAT datasets up to the current version in Pangaea format, (from Karl, or arranged from Karl’s files).</div></li><li><div>another zip file with the dataset bundles in SOCAT format. <b><i>(I’m not sure yet how I generated it??)</i></b></div></li></ul><li><div>Add a special comment in 4D: “List of authors is not the same as the children's”.</div></li></ul><div><br/></div><div><span style="font-weight: bold;">Metadata dataset</span></div><div>It includes in</div><div>Metadata dataset: <a href="https://drive.google.com/open?id=1aBzS6K_L9h8kI9gV_FzbO3RI79hbZim8andBf5nloZU" rev="en_rl_minimal">https://drive.google.com/open?id=1aBzS6K_L9h8kI9gV_FzbO3RI79hbZim8andBf5nloZU</a></div><div><br/></div><div>The basis, campaign, and initial and last positions of the events, are not added via import file. Once imported in 4D, go to the “Configuration” tab, and add those 6 variables in the order shown below:</div><div><br/></div><div><img src="Archive%20SOCAT%20in%20Pangaea.html.resources/1DBE8F79-CF31-4103-8D33-EAE324A99E75.png" height="1128" width="1338"/><br/></div><div>There will be a conflict: the metadata dataset will be not validated until publication, the events included in previous versions will have been published already. Contact Stefi via ticket to solve it.</div><div><br/></div><div>Add the metadata dataset as “Further information” in the parent.</div><div><br/></div><div><span style="font-weight: bold;">Publication</span></div><div>Send the links to the parent and metadata datasets to Benjamin and Dorothee for a last check. Publish.</div><div><br/></div></body></html>